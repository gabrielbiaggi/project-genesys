# testar_gpu_real.py - Teste REAL se llama-cpp-python estÃ¡ usando GPU

print("ğŸ§ª TESTE REAL DE GPU PARA LLAMA-CPP-PYTHON")
print("=" * 50)

try:
    import llama_cpp
    print("âœ… llama-cpp-python importado com sucesso")
    print(f"ğŸ“‹ VersÃ£o: {llama_cpp.__version__}")
    
    # Teste 1: Verificar se foi compilado com CUDA
    print("\nğŸ” TESTE 1: Verificando compilaÃ§Ã£o CUDA...")
    
    # Tentar criar um modelo com GPU
    try:
        # Criar modelo dummy para testar GPU
        print("ğŸ§ª Tentando inicializar com GPU...")
        
        # Verifica se llama_cpp tem suporte a GPU layers
        if hasattr(llama_cpp, 'Llama'):
            print("âœ… Classe Llama disponÃ­vel")
            
            # Verificar se n_gpu_layers estÃ¡ disponÃ­vel
            import inspect
            llama_init_signature = inspect.signature(llama_cpp.Llama.__init__)
            if 'n_gpu_layers' in llama_init_signature.parameters:
                print("âœ… ParÃ¢metro n_gpu_layers DISPONÃVEL (GPU Support)")
                print("ğŸš RESULTADO: llama-cpp-python COM suporte GPU!")
            else:
                print("âŒ ParÃ¢metro n_gpu_layers NÃƒO DISPONÃVEL")
                print("ğŸŒ RESULTADO: llama-cpp-python APENAS CPU!")
                
        else:
            print("âŒ Classe Llama nÃ£o encontrada")
            
    except Exception as e:
        print(f"âŒ Erro ao testar GPU: {e}")
        
    # Teste 2: Verificar bibliotecas CUDA carregadas
    print("\nğŸ” TESTE 2: Verificando bibliotecas carregadas...")
    import sys
    
    cuda_libs_found = False
    for module_name in sys.modules:
        if 'cuda' in module_name.lower():
            print(f"âœ… Biblioteca CUDA carregada: {module_name}")
            cuda_libs_found = True
            
    if not cuda_libs_found:
        print("âš ï¸ Nenhuma biblioteca CUDA carregada")
        
    # Teste 3: Verificar se pode acessar GPU info
    print("\nğŸ” TESTE 3: Testando acesso a informaÃ§Ãµes GPU...")
    try:
        # Tentar acessar informaÃ§Ãµes especÃ­ficas de GPU se disponÃ­vel
        if hasattr(llama_cpp, 'llama_backend_init'):
            print("âœ… Backend init disponÃ­vel")
        
        print("\nğŸ“Š RESULTADO FINAL:")
        
        # VerificaÃ§Ã£o definitiva
        llama_init_signature = inspect.signature(llama_cpp.Llama.__init__)
        if 'n_gpu_layers' in llama_init_signature.parameters:
            print("ğŸ‰ STATUS: GPU ATIVADA!")
            print("âš¡ Performance esperada: 50-200+ tokens/segundo")
            print("âœ… Pode usar n_gpu_layers=-1 para mÃ¡xima performance")
        else:
            print("ğŸŒ STATUS: APENAS CPU!")
            print("ğŸ˜´ Performance esperada: 1-5 tokens/segundo")
            print("âŒ RecompilaÃ§Ã£o com GPU necessÃ¡ria")
            
    except Exception as e:
        print(f"âš ï¸ Erro no teste final: {e}")
        
except ImportError as e:
    print(f"âŒ ERRO: NÃ£o foi possÃ­vel importar llama_cpp: {e}")
    print("ğŸ”§ llama-cpp-python nÃ£o estÃ¡ instalado corretamente")

print("\n" + "=" * 50)