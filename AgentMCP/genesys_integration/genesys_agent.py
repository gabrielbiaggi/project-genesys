# genesys_integration/genesys_agent.py
"""
Agente Genesys integrado ao Agent-MCP
Executa o modelo LLaMA 70B com processamento de imagens localmente
"""
import os
import sys
import asyncio
import json
import base64
import threading
import time
from typing import Dict, List, Any, Optional
from pathlib import Path
from dotenv import load_dotenv

import google.generativeai as genai

# ImportaÃ§Ãµes para o modelo LLaMA
try:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from langchain_community.llms import LlamaCpp
    from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler
    LLAMA_AVAILABLE = True
except ImportError:
    LLAMA_AVAILABLE = False
    print("âš ï¸ DependÃªncias do LLaMA nÃ£o instaladas. Use: pip install -r requirements_genesys.txt")

# ImportaÃ§Ãµes das ferramentas da Genesys original
try:
    sys.path.append(str(Path(__file__).parent.parent / "C:\\DEVBill\\Projetos\\Genesys"))
    from app.tools.file_system_tool import apply_file_change, list_files, read_file
    from app.tools.terminal_tool import execute_terminal_command
    from app.tools.web_search_tool import web_search
    GENESYS_TOOLS_AVAILABLE = True
except ImportError:
    GENESYS_TOOLS_AVAILABLE = False
    print("âš ï¸ Ferramentas da Genesys nÃ£o encontradas. Usando versÃµes mockadas.")

import re
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

from .tools import TOOL_DEFINITIONS, execute_tool

# --- Prompts ---

ERROR_CORRECTION_PROMPT = """
VocÃª Ã© um especialista em depuraÃ§Ã£o de software. Uma ferramenta executada anteriormente falhou.
Sua tarefa Ã© analisar o erro e a tarefa original para fornecer uma soluÃ§Ã£o.

Tarefa Original:
{original_task}

Ferramenta Invocada (formato JSON):
{tool_call_json}

Erro Recebido:
{error_message}

Contexto Adicional:
- O ambiente de execuÃ§Ã£o Ã© Windows.
- As ferramentas disponÃ­veis incluem um sistema de arquivos (`read_file`, `list_files`, `write_file`) e pesquisa na web (`web_search`).

AnÃ¡lise e Plano de AÃ§Ã£o:
1.  **DiagnÃ³stico**: Qual Ã© a causa provÃ¡vel do erro? (Ex: arquivo nÃ£o encontrado, comando invÃ¡lido, problema de permissÃ£o, etc.)
2.  **Pesquisa (se necessÃ¡rio)**: Se a causa nÃ£o for Ã³bvia, formule uma consulta de pesquisa para a ferramenta `web_search` para encontrar soluÃ§Ãµes.
3.  **SoluÃ§Ã£o Proposta**: Com base na sua anÃ¡lise e/ou pesquisa, proponha uma nova chamada de ferramenta corrigida ou uma abordagem alternativa para cumprir a tarefa original. A resposta DEVE ser uma chamada de ferramenta em formato JSON, como `{"tool": "web_search", "args": {"query": "como resolver X"}}` ou `{"tool": "read_file", "args": {"file_path": "C:\\path\\correto\\arquivo.txt"}}`.
4.  **Se Nenhuma SoluÃ§Ã£o For Encontrada**: Se nÃ£o for possÃ­vel corrigir, responda com uma explicaÃ§Ã£o clara do problema e por que nÃ£o pode ser resolvido.

Sua resposta final deve ser APENAS a chamada de ferramenta corrigida em JSON ou a explicaÃ§Ã£o.
"""

class GenesysAgent:
    """O agente principal que carrega um modelo local e interage com ferramentas."""
    
    def __init__(self, model_path: str = None, use_gpu: bool = True):
        self.agent_id = "genesys_master"
        self.specializations = ["coding", "multimodal", "system_integration", "ai_orchestration"]
        self.model_path = model_path or self._get_model_path()
        self.use_gpu = use_gpu
        self.model = None
        self.tokenizer = None
        self.llama_cpp = None
        self.is_loaded = False
        self.tools = self._setup_tools()
        self.gemini_model = None
        
        # Carregar variÃ¡veis de ambiente
        load_dotenv()
        
        # Configurar Gemini
        gemini_api_key = os.getenv("GEMINI_API_KEY")
        if gemini_api_key:
            try:
                genai.configure(api_key=gemini_api_key)
                self.gemini_model = genai.GenerativeModel('gemini-1.5-pro-latest')
                print("âœ… API do Gemini configurada com sucesso!")
            except Exception as e:
                print(f"âš ï¸ Erro ao configurar a API do Gemini: {e}")
        else:
            print("âš ï¸ GEMINI_API_KEY nÃ£o encontrada no .env. FunÃ§Ãµes avanÃ§adas desativadas.")
        
    def _get_model_path(self) -> str:
        """Determina o caminho do modelo baseado na configuraÃ§Ã£o"""
        # Primeiro, tentar usar o modelo da Genesys original
        genesys_model = os.getenv("MODEL_GGUF_FILENAME", "llama-3-70b-instruct.Q4_K_M.gguf")
        genesys_path = f"C:\\DEVBill\\Projetos\\Genesys\\models\\{genesys_model}"
        
        if os.path.exists(genesys_path):
            return genesys_path
        
        # Fallback para modelo local no projeto
        local_model = Path(__file__).parent / "models" / genesys_model
        if local_model.exists():
            return str(local_model)
        
        # Se nÃ£o encontrar, usar o modelo padrÃ£o
        return f"./genesys_integration/models/{genesys_model}"
    
    def _setup_tools(self) -> Dict[str, Any]:
        """Carrega e configura as ferramentas disponÃ­veis."""
        self.tool_definitions = TOOL_DEFINITIONS
        self.tool_executor = execute_tool
        # Mapeamento para fÃ¡cil acesso, se necessÃ¡rio
        self.tools = {tool['name']: tool for tool in self.tool_definitions}
        print(f"âœ… Ferramentas configuradas: {', '.join(self.tools.keys())}")
        return self.tools
    
    def _create_mock_tools(self) -> Dict[str, Any]:
        """DEPRECATED: Mantido para compatibilidade, mas _setup_tools Ã© o preferido."""
        print("âš ï¸ _create_mock_tools estÃ¡ obsoleto. Usando _setup_tools.")
        return self._setup_tools()
    
    async def load_model(self) -> bool:
        """Carrega o modelo de linguagem local (GGUF) usando LlamaCpp."""
        if not LLAMA_AVAILABLE:
            print("âŒ DependÃªncias do LLaMA nÃ£o disponÃ­veis")
            return False
        
        if self.is_loaded:
            return True
        
        print(f"ðŸ¤– Carregando Genesys (LLaMA 70B)...")
        print(f"ðŸ“ Caminho do modelo: {self.model_path}")
        
        try:
            # Usar llama-cpp-python para melhor performance com GPU
            if os.path.exists(self.model_path):
                self.llama_cpp = LlamaCpp(
                    model_path=self.model_path,
                    n_gpu_layers=-1 if self.use_gpu else 0,
                    n_batch=512,
                    n_ctx=4096,
                    f16_kv=True,
                    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
                    verbose=False,
                    temperature=0.7,
                )
                self.is_loaded = True
                print("âœ… Genesys carregada com sucesso!")
                return True
            else:
                print(f"âŒ Modelo nÃ£o encontrado em: {self.model_path}")
                return False
                
        except Exception as e:
            print(f"âŒ Erro ao carregar modelo: {e}")
            return False
            
    def _should_use_gemini(self, task: str) -> bool:
        """Determina se a tarefa Ã© complexa o suficiente para usar a API do Gemini."""
        if not self.gemini_model:
            return False
        
        complex_keywords = [
            "analise", "refatore", "arquitetura", "explique detalhadamente", 
            "crie um plano", "projete", "melhore este cÃ³digo", "auditoria",
            "implemente a seguinte feature", "escreva um teste completo"
        ]
        
        task_lower = task.lower()
        return any(keyword in task_lower for keyword in complex_keywords)

    async def _call_gemini_api(self, prompt: str) -> str:
        """Chama a API do Gemini com o prompt fornecido."""
        print("ðŸ§  Tarefa complexa detectada. Usando API do Gemini...")
        try:
            response = await self.gemini_model.generate_content_async(prompt)
            return response.text
        except Exception as e:
            print(f"âŒ Erro ao chamar a API do Gemini: {e}")
            return f"Erro ao processar com Gemini: {e}"
    
    async def process_task(self, task: str, context: Dict = None, use_tools: bool = True) -> str:
        """Processa uma tarefa usando o modelo Genesys ou a API do Gemini."""
        
        specialized_prompt = self._create_specialized_prompt(task, context, use_tools)
        
        # LÃ³gica de decisÃ£o: Usar Gemini para tarefas complexas
        if self._should_use_gemini(task):
            return await self._call_gemini_api(specialized_prompt)

        # LÃ³gica original para usar o modelo local
        if not self.is_loaded:
            await self.load_model()
        
        if not self.is_loaded:
            return "âŒ Modelo Genesys nÃ£o estÃ¡ carregado"
        
        try:
            # Usar llama-cpp-python
            response = self.llama_cpp(specialized_prompt)
            
            # Se a resposta indica uso de ferramenta, executar
            if use_tools and self._should_use_tools(response):
                tool_response = await self._execute_tools(response, task)
                return f"{response}\n\nðŸ”§ **Resultado das Ferramentas:**\n{tool_response}"
            
            return response
            
        except Exception as e:
            return f"âŒ Erro no processamento local: {str(e)}"
    
    def _create_specialized_prompt(self, task: str, context: Dict = None, use_tools: bool = True) -> str:
        """Cria prompt especializado baseado no contexto"""
        base_prompt = f"""ðŸ¤– **AGENTE GENESYS MASTER**
ID: {self.agent_id}
EspecializaÃ§Ãµes: {', '.join(self.specializations)}

CONTEXTO DO PROJETO: Agent-MCP - Sistema de orquestraÃ§Ã£o multi-agente
MEU PAPEL: Agente especialista master com modelo LLaMA 70B local

TAREFA RECEBIDA: {task}

CONTEXTO ADICIONAL: {json.dumps(context, indent=2) if context else 'Nenhum'}

FERRAMENTAS DISPONÃVEIS: {'Ativadas' if use_tools else 'Desativadas'}
- file_system: Modificar arquivos
- list_files: Listar diretÃ³rios
- read_file: Ler arquivos
- terminal: Executar comandos
- web_search: Buscar na web

INSTRUÃ‡Ã•ES:
1. Analise a tarefa cuidadosamente
2. Use ferramentas quando necessÃ¡rio
3. ForneÃ§a respostas tÃ©cnicas detalhadas
4. Coordene com outros agentes via Agent-MCP
5. Mantenha foco em cÃ³digo e implementaÃ§Ã£o

RESPOSTA:
"""
        return base_prompt
    
    def _should_use_tools(self, response: str) -> bool:
        """Determina se a resposta indica necessidade de usar ferramentas"""
        tool_indicators = [
            "usar ferramenta", "executar comando", "ler arquivo", 
            "modificar arquivo", "buscar na web", "listar arquivos"
        ]
        return any(indicator in response.lower() for indicator in tool_indicators)
    
    async def _execute_tools(self, response: str, original_task: str) -> str:
        """Analisa a resposta do modelo, executa a ferramenta e retorna o resultado."""
        try:
            # Extrai o JSON da chamada da ferramenta da resposta do modelo
            tool_call_match = re.search(r'\{.*\}', response, re.DOTALL)
            if not tool_call_match:
                return "NÃ£o foi possÃ­vel extrair a chamada da ferramenta da resposta."

            tool_call_json = tool_call_match.group(0)
            tool_data = json.loads(tool_call_json)
            tool_name = tool_data.get("tool")
            tool_args = tool_data.get("args", {})

            if not tool_name:
                return "Nome da ferramenta nÃ£o especificado na chamada."

            print(f"â–¶ï¸ Executando ferramenta: {tool_name} com args: {tool_args}")
            # A funÃ§Ã£o execute_tool agora Ã© assÃ­ncrona
            result = await self.tool_executor(tool_name, tool_args)
            print(f"â—€ï¸ Resultado da ferramenta: {result[:200]}...")
            return result

        except Exception as e:
            print(f"âŒ Erro ao executar a ferramenta: {e}")

            # --- CICLO DE AUTO-CORREÃ‡ÃƒO ---
            if self.gemini_model:
                print("ðŸ§  Iniciando ciclo de auto-correÃ§Ã£o com a API do Gemini...")
                
                # Tenta obter o JSON da chamada da ferramenta, mesmo que tenha falhado
                tool_call_json_for_prompt = "N/A"
                try:
                    tool_call_json_for_prompt = json.dumps(json.loads(re.search(r'\{.*\}', response, re.DOTALL).group(0)))
                except:
                    pass # MantÃ©m "N/A" se a extraÃ§Ã£o falhar

                correction_prompt = ERROR_CORRECTION_PROMPT.format(
                    original_task=original_task,
                    tool_call_json=tool_call_json_for_prompt,
                    error_message=str(e)
                )

                # Chama o Gemini para obter uma sugestÃ£o de correÃ§Ã£o
                correction_suggestion = await self._call_gemini_api(correction_prompt)
                
                print(f"ðŸ’¡ SugestÃ£o de correÃ§Ã£o do Gemini: {correction_suggestion}")
                
                # Verifica se a sugestÃ£o Ã© uma nova chamada de ferramenta e a executa
                if 'tool' in correction_suggestion and 'args' in correction_suggestion:
                    print("ðŸ” Tentando executar a chamada de ferramenta corrigida...")
                    # Chama a si mesmo recursivamente com a sugestÃ£o corrigida
                    return await self._execute_tools(correction_suggestion, original_task)
                else:
                    # Se a sugestÃ£o nÃ£o for uma ferramenta, retorna a explicaÃ§Ã£o do Gemini
                    return f"AnÃ¡lise do Gemini sobre o erro: {correction_suggestion}"

            return f"Erro final ao executar a ferramenta (sem auto-correÃ§Ã£o): {e}"

    async def process_multimodal(self, text: str, image_data: str = None) -> str:
        """Processa entradas multimodais (texto e imagem)."""
        if not image_data:
            return await self.process_task(text)
        
        # Para processamento de imagem, vocÃª precisaria integrar com LLaVA ou similar
        # Por agora, retornamos processamento apenas de texto
        enhanced_prompt = f"""
ðŸ–¼ï¸ **PROCESSAMENTO MULTIMODAL**

TEXTO: {text}
IMAGEM: {'Fornecida (Base64)' if image_data else 'NÃ£o fornecida'}

AnÃ¡lise da imagem fornecida juntamente com o texto...
"""
        return await self.process_task(enhanced_prompt)
    
    def get_status(self) -> Dict[str, Any]:
        """Retorna status atual do agente"""
        return {
            "agent_id": self.agent_id,
            "specializations": self.specializations,
            "model_loaded": self.is_loaded,
            "model_path": self.model_path,
            "gpu_enabled": self.use_gpu,
            "tools_available": GENESYS_TOOLS_AVAILABLE,
            "llama_available": LLAMA_AVAILABLE
        }

# InstÃ¢ncia global do agente
genesys_agent = GenesysAgent()

async def main():
    """FunÃ§Ã£o principal para testes"""
    print("ðŸ§ª Testando Agente Genesys...")
    
    # Carregar modelo
    await genesys_agent.load_model()
    
    # Teste bÃ¡sico
    response = await genesys_agent.process_task(
        "Analise a estrutura do projeto Agent-MCP e sugira melhorias"
    )
    print(f"Resposta: {response}")
    
    # Status
    status = genesys_agent.get_status()
    print(f"Status: {json.dumps(status, indent=2)}")

if __name__ == "__main__":
    asyncio.run(main())
